# ALFWorld Training Configuration
# Use with: python alfworld_client.py

# Project settings
project_name: opentinker
experiment_name: alfworld_training

# Logging
logger_backends: ["console", "wandb"]

# Tracing (optional)
enable_tracing: true
weave_project: null

# WandB (optional)
wandb_key: null

# Model and tokenizer
tokenizer_path: null

# Training parameters
batch_size: 4
num_workers: 4
# Training duration - set ONE of these (num_steps takes precedence if both set)
num_epochs: null  # Number of epochs (null = use num_steps)
num_steps: 1000   # Total training steps (null = use num_epochs)
save_freq: 20000
test_freq: 10     # Validation frequency (every N steps)

# Validation parameters
val_batch_size: 50    # Total validation samples (null = 50)

# Generation parameters
temperature: 1  # Lower temperature for more focused responses
top_p: 1
max_new_tokens: 4096  # TOTAL response budget for entire multi-turn trajectory
max_prompt_tokens: 2048

# Algorithm (must be agent_loop for multi-turn)
algorithm: "agent_loop"

# RL Algorithm settings (passed to server via scheduler)
adv_estimator: "grpo"
rollout_n: 8

# Interaction configuration
interaction:
  name: alfworld
  class_path: opentinker.environment.gym_environment_interaction.GymEnvironmentInteraction
  config:
    env_host: 0.0.0.0
    env_port: 8082
    env_endpoint: http://${interaction.config.env_host}:${interaction.config.env_port}
    max_steps: 50  # Max steps for GymEnvironmentInteraction
    max_total_steps: 50  # Max environment step() calls
    observation_template: "{observation}"

multi_turn:
    max_user_turns: ${interaction.config.max_total_steps}
    max_assistant_turns: ${interaction.config.max_total_steps}
    max_tokens_per_turn: 256  # Per-turn response limit
    weave_project: null
    experiment_name: "alfworld_interaction"

# Scheduler settings
scheduler_url: "http://0.0.0.0:8780"
scheduler_api_key: otk_98b8db24ccd64c92e1fdd9a232e209fa

# GPU settings
num_gpus: 2

